############## DEFAULT CONFIGURATION ##########
defaults:
- data: zarr
- dataloader: native_grid
- diagnostics: evaluation
- hardware: slurm
- graph: stretched_grid
- model: graphtransformer
- training: default
- _self_

################# DATA ########################
data:
  resolution: ""
  prognostic:
  forcing:
  - "lsm"
  - "z_sfc"
  - "insolation"
  - "cos_julian_day"
  - "cos_local_time"
  - "cos_longitude"
  - "cos_latitude"
  - "sin_julian_day"
  - "sin_local_time"
  - "sin_longitude"
  - "sin_latitude"
  diagnostic:
  - "tp"
  normalizer:
    default: "mean-std"
    std:
    - "tp"
    min-max:
    max:
    none:
    - "lsm"
    - "z_sfc"
    - "insolation"
    - "cos_julian_day"
    - "cos_local_time"
    - "cos_longitude"
    - "cos_latitude"
    - "sin_julian_day"
    - "sin_local_time"
    - "sin_longitude"
    - "sin_latitude"

################# DATALOADER ##################
dataloader:
  dataset: 
    cutout:
      - dataset: ${hardware.paths.data}/${hardware.files.dataset}
        thinning: 1
      - dataset: ${hardware.paths.data}/${hardware.files.forcing_dataset}
    adjust: all
    min_distance_km: 3
    # min_distance_km: 0
  # Number of workers and batch size. 
  #   Global training batch size is equal to num_workers * per-gpu batch size. 
  num_workers:
    training: 1 # 8
    validation: 1 # 8
    test: 1 # 8
    predict: 1 # 8
  batch_size:
    training: 1
    validation: 1
    test: 1
    predict: 1
  training:
    start: 2012-09-01
    end: 2020-05-29
    # end: 2012-09-03
    statistics: ${hardware.paths.data}/era5-31km.zarr
    drop: [q_800, t_800, z_800, u_800, v_800]
  validation:
    start: 2020-05-30
    end: 2020-05-31
    statistics: ${hardware.paths.data}/era5-31km.zarr
    drop: [q_800, t_800, z_800, u_800, v_800]


################# DIAGNOSTICS #################
diagnostics:
  defaults:
    - plot: none
    - callbacks: pretraining
    - benchmark_profiler: simple
  # Save the models during training (checkpointing).
  #   This is related to "hardware", where is indicated the names of the checkpointed models
  enable_checkpointing: True
  checkpoint:
    every_n_minutes:
      save_frequency: 30 # Approximate, as this is checked at the end of training steps
      num_models_saved: 0 # If set to k, saves the 'last' k model weights in the training.
    every_n_epochs:
      save_frequency: 1
      num_models_saved: -1 # If set to -1, all checkpoints are kept ensuring runs can be continued/forked at any point in the training process
    every_n_train_steps:
      save_frequency: null # Does not scale with rollout
      num_models_saved: 0
  log:
    mlflow:
      enabled: False
      offline: False
      authentication: False
      log_model: True
      # tracking_uri: http://127.0.0.1:3326
      experiment_name: 'anemoi-debug'
      project_name: 'Anemoi'
      system: True
      terminal: True
      run_name: null # If set to null, the run name will be the a random UUID
      on_resume_create_child: True
      expand_hyperparams: # Which keys in hyperparams to expand
        - config
    interval: 100
  
################# HARDWARE ####################
hardware:
  # Number of GPUs
  num_gpus_per_node: 4
  num_nodes: 16
  num_gpus_per_model: 4
  # Paths
  paths:
    data: /work/nvme/bduu/jbanomedina/regional-ai/data/zarrs/
    grids: /work/nvme/bduu/jbanomedina/regional-ai/graphs/STRETCHED-6km/
    output: /work/nvme/bduu/jbanomedina/regional-ai/training/STRETCHED-6km/logs/outputs_
  files:
    dataset: cw3e-6km-train.zarr # cw3e-6km-train.zarr # test-1979-01-cw3e-6km-allvars.zarr # cw3e-6km-train.zarr
    forcing_dataset: era5-31km.zarr # era5-31km.zarr # test-1979-01-era5-31km-allvars.zarr # era5-31km.zarr
    graph: STRETCHED-6km-9x7-12x3.pt
    checkpoint:
      every_n_epochs: stage-B1-by_epoch-epoch_{epoch:03d}-step_{step:06d}
      every_n_train_steps: stage-B1-by_step-epoch_{epoch:03d}-step_{step:06d}
      every_n_minutes: stage-B1-by_time-epoch_{epoch:03d}-step_{step:06d}
    warm_start: null # last.ckpt # or last.ckpt or null. last.ckpt to retrain a pre-trained model

################# GRAPH #######################
graph:
  overwrite: False
  nodes:
    data:
      node_builder:
        _target_: anemoi.graphs.nodes.ZarrDatasetNodes
        dataset: ${dataloader.training.dataset}
      attributes: ${graph.attributes.nodes}
    hidden:
      node_builder:
        _target_: anemoi.graphs.nodes.StretchedTriNodes
        lam_resolution: 9
        global_resolution: 7
        reference_node_name: ${graph.data}
        mask_attr_name: cutout
        margin_radius_km: 11
  edges:
  # Encoder
  - source_name: ${graph.data}
    target_name: ${graph.hidden}
    edge_builders:
    - _target_: anemoi.graphs.edges.KNNEdges
      num_nearest_neighbours: 12
    attributes: ${graph.attributes.edges}
  # Processor
  - source_name: ${graph.hidden}
    target_name: ${graph.hidden}
    edge_builders:
    - _target_: anemoi.graphs.edges.MultiScaleEdges
      x_hops: 1
    attributes: ${graph.attributes.edges}
  # Decoder
  - source_name: ${graph.hidden}
    target_name: ${graph.data}
    edge_builders:
    - _target_: anemoi.graphs.edges.KNNEdges
      num_nearest_neighbours: 3
    attributes: ${graph.attributes.edges}
################# MODEL #######################
model: # What are trainable parameters?!!!
  num_channels: 512
  processor:
    num_layers: 16
  bounding: 
    - _target_: anemoi.models.layers.bounding.ReluBounding #[0, infinity)
      variables:
      - tp
  trainable_parameters:
    data: 0
    hidden: 0
    data2hidden: 0
    hidden2data: 0
    hidden2hidden: 0 # GNN and GraphTransformer Processor only

################# TRAINING ####################
training:  # Research about loss scaling and metrics!!!
  # Start the training from a pre-trained model. 
  #   resume or fork a training from a checkpoint last.ckpt or specified in hardware.files.warm_start
  fork_run_id: source
  load_weights_only: True
  transfer_learning: True #  Activate to perform transfer learning
  # Epochs, steps and local learning rate.
  max_epochs: null
  max_steps: 25000 # 25000
  lr: 
    rate: 4e-6 # 8e-6 # local_lr. global_lr = local_lr * num_gpus_per_node * num_nodes / gpus_per_model
    iterations: ${training.max_steps} # NOTE: When max_epochs < max_steps, scheduler will run for max_steps
    min: 3e-7 # Not scaled by #GPU
    warmup_t: 1000
  # Rollout
  rollout:
    start: 1 # Is this the parameter to increase the rollout?
    epoch_increment: 0 # increase rollout every n epochs
    max: 1 # maximum rollout to use
  # Check dataset before training?
  num_sanity_val_steps: 0
  multistep_input: 2
  # Relative importance of variables in the loss function
  loss_scaling:
    spatial:
      _target_: anemoi.training.data.scaling.ReweightedGraphAttribute
      target_nodes: ${graph.data}
      scaled_attribute: area_weight # it must be a node attribute of the output nodes
      cutout_weight_frac_of_global: 0.25
  variable_loss_scaling:
    default: 1
    pl:
      q: 0.6 #1
      t: 6   #1
      u: 0.8 #0.5
      v: 0.5 #0.33
      z: 12  #1
    sfc:
      sp: 10
      10u: 0.1
      10v: 0.1
      2d: 0.5
      tp: 0.025
  metrics:
  # - ivt_u
  # - ivt_v
  # - tp
  - 2t





